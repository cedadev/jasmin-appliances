---
#####
## Playbook that provisions and then configures a Kubernetes
## cluster on OpenStack
#####

# Provision the cluster infrastructure
- hosts: openstack
  tasks:
    - import_tasks: tasks/install-dependencies.yml

    # Add the identity stack to the cluster and configure it as the gateway
    - include_role:
        name: jasmin.cluster-infra
      vars:
        cluster_name: "{{ identity_stack_name }}"
        cluster_stack_update: false
        cluster_gw_group: "{{ identity_gw_group_name }}"

    # Provision the Kubernetes infrastructure
    - import_tasks: tasks/infra/provision.yml
      vars:
        # These are the groups to use when no fixed IP is given
        cluster_groups: "{{ k8s_groups_no_ip }}"
        # These are the groups to use when a fixed IP is given
        cluster_groups_fixed_ip: "{{ k8s_groups_fixed_ip }}"
        # Tag to assign to the cluster
        cluster_tag: "{{ cluster_type.kubernetes }}"

- hosts: cluster
  become: true
  tasks:
    - import_tasks: tasks/network/hosts.yml

- hosts: rke_hosts
  become: true
  tasks:
    # Make sure that we have an Openstack trust
    # It is important to do this early, before the token expires
    # This is created on the Openstack host but persisted on the cluster nodes
    # These tasks set the openstack_trust_id and openstack_project_id variables
    - include_tasks: tasks/util/os_trust.yml
    - include_tasks: tasks/kubernetes/upgrade_os_packages.yml
    # Before installing Kubernetes, enroll as a FreeIPA client
    - include_tasks: tasks/identity/freeipa/client.yml

# Set the permissions for the cluster in FreeIPA
- hosts: freeipa_servers
  become: true
  tasks:
    - include_tasks: tasks/identity/freeipa/cluster_permissions.yml
      vars:
        # Create a users group, but don't grant SSH access
        # We will hook this group up to the cluster-admin role
        create_users_group: true

# Create/delete the OIDC client for the cluster
- hosts: keycloak_servers
  become: true
  tasks:
    - name: Set Keycloak redirect URIs
      set_fact:
        # Include redirect URIs for use with kubectl and the dashboards when enabled
        keycloak_client_redirect_uris: >-
          [
            'http://localhost:8000',
            'urn:ietf:wg:oauth:2.0:oob',
            'https://{{ hostvars[groups.rke_masters | first].kubernetes_dashboard_domain }}/oauth/callback',
          ]
      when: cluster_state | default('present') == 'present'

    - include_tasks: tasks/identity/keycloak/oidc_client.yml
      vars:
        keycloak_client_id: "{{ cluster_name }}"
        keycloak_client_name: "Kubernetes cluster - {{ cluster_name }}"
        keycloak_client_state: "{{ cluster_state | default('present') }}"

# Install Kubernetes
- hosts: rke_hosts
  become: true
  any_errors_fatal: true
  pre_tasks:
    - name: Set RKE node roles (masters)
      set_fact:
        rke_node_roles: [etcd, controlplane]
      when: inventory_hostname in groups.rke_masters

    - name: Set RKE node roles (workers)
      set_fact:
        rke_node_roles: [worker]
      when: inventory_hostname in groups.rke_workers

    # Fix required for now - enable IP masquerading in firewalld for the tenancy network
    # https://github.com/rancher/rancher/issues/28840#issuecomment-756714369
    - name: Enable IP masquerading
      ansible.posix.firewalld:
        masquerade: "yes"
        zone: "{{ item }}"
        state: enabled
        permanent: yes
        immediate: yes
      loop: [public, trusted]
  roles:
    # Install Kubernetes using RKE
    - role: jasmin.rke
      vars:
        # In order to consume a single floating IP, we want to schedule the ingress controllers
        # on the controlplane nodes
        # However RKE doesn't allow us to specify tolerations
        # So we don't deploy ingress with RKE at all and deploy with Helm later
        rke_ingress_config: { provider: none }
        # Use the centos user with the cluster SSH key
        rke_ssh_user: centos
        rke_ssh_private_key: "{{ lookup('ansible.builtin.file', ansible_ssh_private_key_file) }}"
        rke_cloud_provider: openstack
        # Configure the API server to authenticate using Keycloak
        rke_extra_config:
          services:
            kube-api:
              extra_args:
                oidc-issuer-url: "{{ hostvars[groups.keycloak_servers | first].keycloak_oidc_issuer_url }}"
                oidc-client-id: "{{ hostvars[groups.keycloak_servers | first].keycloak_client_id }}"
                oidc-username-claim: preferred_username
                oidc-username-prefix: "oidc:"
                oidc-groups-claim: groups
                oidc-groups-prefix: "oidc:"
        # Grant the cluster-admin role to the global admins and cluster admins groups
        rke_extra_addons: |
          ---
          apiVersion: rbac.authorization.k8s.io/v1
          kind: ClusterRoleBinding
          metadata:
            name: oidc-admins-cluster-admin
          roleRef:
            apiGroup: rbac.authorization.k8s.io
            kind: ClusterRole
            name: cluster-admin
          subjects:
            - apiGroup: rbac.authorization.k8s.io
              kind: Group
              name: oidc:admins
            - apiGroup: rbac.authorization.k8s.io
              kind: Group
              name: oidc:{{ hostvars[groups.freeipa_servers | first].user_group_name }}

- hosts: rke_hosts
  become: true 
  tasks:
    - name: Create/update cloud provider config
      template:
        src: templates/external-cloud-provision/cloud.conf
        dest: "/etc/kubernetes/cloud.conf"
        mode: u=rw,g=r,o=
      register: provider_config

- hosts: rke_masters
  become: true
  tasks:
    # The CIDRs from the last time we ran will be in ansible_local
    # Compare them to the current CIDRs and adjust as necessary
    - name: Open Kubernetes API for given CIDRs
      ansible.posix.firewalld:
        rich_rule: >-
          rule family="ipv4"
          source address="{{ item }}"
          port port="6443" protocol="tcp" accept
        permanent: true
        immediate: true
        state: "{{ 'enabled' if item in admin_allowed_cidrs else 'disabled' }}"
      loop: "{{ ansible_local.admin_allowed_cidrs | default([]) | union(admin_allowed_cidrs) | unique | list }}"

    # Store the current allowed CIDRs for next time
    - name: Ensure facts.d directory exists
      file:
        path: /etc/ansible/facts.d
        state: directory

    - name: Write facts.d file containing allowed CIDRs
      copy:
        content: "{{ admin_allowed_cidrs | to_json }}"
        dest: /etc/ansible/facts.d/admin_allowed_cidrs.fact
        mode: "u=rw,g=,o="

    - include_role:
        name: jasmin.helm

    - name: Install Helm repositories
      command: helm repo add --force-update {{ item.name }} {{ item.url }}
      loop:
        - name: stable
          url: https://charts.helm.sh/stable
        - name: jetstack
          url: https://charts.jetstack.io
        - name: cedadev
          url: https://cedadev.github.io/helm-charts
        - name: ingress-nginx
          url: https://kubernetes.github.io/ingress-nginx
        - name: cpo
          url: https://kubernetes.github.io/cloud-provider-openstack
      loop_control:
        label: "{{ item.name }}"

    # Install the Nginx ingress controller first
    - name: Install Nginx ingress controller
      include_role:
        name: jasmin.helm
        tasks_from: chart.yml
      vars:
        helm_release_name: nginx-ingress
        helm_release_chart: ingress-nginx/ingress-nginx
        helm_release_version: "4.2.0"
        helm_release_namespace: nginx-ingress
        helm_release_values_template: templates/helm/nginx-ingress.yml

    # Open ports 80 and 443 for the ingress controller
    - name: Open HTTP(S) ports for ingress controller
      ansible.posix.firewalld:
        service: "{{ item }}"
        permanent: true
        immediate: true
        state: enabled
      loop: [http, https]

    - name: Detect cert-manager version
      shell: helm list --all-namespaces | grep cert-manager | awk '{ print $9; }' | awk -F"-" '{ print $3; }'
      changed_when: false
      register: cert_manager_current_version_cmd

    - name: Set cert-manager current version fact
      set_fact:
        cert_manager_current_version: "{{ cert_manager_current_version_cmd.stdout }}"

    # First step is to upgrade cert-manager to 0.14 where changes happened
    - name: Upgrade cert-manager to v1.8
      block:
        - name: Install cert-manager CRDs for v1.8
          command: kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.8.2/cert-manager.crds.yaml

        - name: Remove cert-manager deployment before upgrade
          command: kubectl delete -n cert-manager deployment cert-manager cert-manager-cainjector cert-manager-webhook

        - name: Install cert-manager v1.8
          include_role:
            name: jasmin.helm
            tasks_from: chart.yml
          vars:
            helm_release_name: cert-manager
            helm_release_chart: jetstack/cert-manager
            helm_release_version: v1.8.2
            helm_release_namespace: cert-manager
            helm_release_values_template: templates/helm/cert-manager.yml

        - name: Update cert-manager current version fact
          set_fact:
            cert_manager_current_version: v1.8.2
      when:
        - "cert_manager_current_version is match('v\\d+\\.\\d+\\.\\d+')"
        - "cert_manager_current_version is version('v0.14', '<')"

    - name: Install cert-manager CRDs
      command: kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/{{ cert_manager_version }}/cert-manager.crds.yaml

    - name: Install cert-manager
      include_role:
        name: jasmin.helm
        tasks_from: chart.yml
      vars:
        helm_release_name: cert-manager
        helm_release_chart: jetstack/cert-manager
        helm_release_version: "{{ cert_manager_version }}"
        helm_release_namespace: cert-manager
        helm_release_values_template: templates/helm/cert-manager.yml

    - name: Configure LetsEncrypt issuer
      shell: |
        kubectl apply -f - <<EOF
        apiVersion: cert-manager.io/v1
        kind: ClusterIssuer
        metadata:
          name: letsencrypt
        spec:
          acme:
            server: https://acme{% if letsencrypt_staging | default(false) | bool %}-staging{% endif %}-v02.api.letsencrypt.org/directory
            privateKeySecretRef:
              name: letsencrypt
            # Enable the HTTP-01 challenge solver
            solvers:
              - http01:
                  ingress:
                    class: nginx
        EOF

    - name: Install Kubernetes dashboard
      command: kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.0/aio/deploy/recommended.yaml

    - name: Create Kubernetes dashboard proxy encryption key
      include_tasks: tasks/util/persistent_random_fact.yml
      vars:
        fact_name: kubernetes_dashboard_proxy_encryption_key
        # 16 bytes is important - this is a 128-bit AES-256 key
        random_fact_bytes: 16
    - name: Install dashboard authenticating proxy
      include_role:
        name: jasmin.helm
        tasks_from: chart.yml
      vars:
        helm_release_name: kubernetes-dashboard-proxy
        helm_release_chart: cedadev/keycloak-gatekeeper
        helm_release_namespace: kubernetes-dashboard
        helm_release_values_template: templates/helm/keycloak-gatekeeper.yml
        helm_release_state: present
        gatekeeper_discovery_url: "{{ hostvars[groups.keycloak_servers | first].keycloak_oidc_issuer_url }}"
        # If using Let's Encrypt staging server, we need to skip TLS verification for the IdP
        gatekeeper_provider_skip_tls_verify: "{{ letsencrypt_staging | default(false) | bool }}"
        gatekeeper_client_id: "{{ hostvars[groups.keycloak_servers | first].keycloak_client_id }}"
        gatekeeper_client_secret: "{{ hostvars[groups.keycloak_servers | first].keycloak_client_secret }}"
        gatekeeper_upstream: https://kubernetes-dashboard
        gatekeeper_encryption_key: "{{ ansible_local.kubernetes_dashboard_proxy_encryption_key }}"
        gatekeeper_domain: "{{ kubernetes_dashboard_domain }}"
        gatekeeper_allowed_cidrs: "{{ admin_allowed_cidrs }}"

    # - name: Create/update OCCM provider config
    #   template:
    #     src: templates/external-cloud-provision/openstack-ccm.yml
    #     dest: "{{ rke_config_dir }}/openstack-ccm.yml"
    #     mode: u=rw,g=r,o=
    #   register: ccm_conf

    # - name: Install Openstack CCM 
    #   include_role:
    #     name: jasmin.helm
    #     tasks_from: chart.yml
    #   vars:
    #     helm_release_name: openstack-ccm 
    #     helm_release_chart: cpo/openstack-cloud-controller-manager
    #     helm_release_namespace: kube-system
    #     helm_release_values_template: templates/external-cloud-provision/openstack-ccm.yml

    - name: Install Cinder CSI Plugin
      include_role:
        name: jasmin.helm
        tasks_from: chart.yml
      vars:
        helm_release_name: cinder-csi
        helm_release_chart: cpo/openstack-cinder-csi
        helm_release_namespace: kube-system
        helm_release_version: "2.26.0"
-       helm_release_values_template: templates/helm/cinder-csi.yml
      

# # Install Openstack cloud controller manager and Cinder CSI provisioner
# - hosts: rke_hosts
#   become: true 
#   tasks:
#   - name: Create/update cloud provider config
#     template:
#       src: templates/external-cloud-provision/cloud.conf
#       dest: "/etc/kubernetes/cloud.conf"
#       mode: u=rw,g=r,o=
#     register: provider_config

# - hosts: rke_hosts
#   become: true 
#   tasks:
#   - name: Create/update OCCM provider config
#     template:
#       src: templates/external-cloud-provision/openstack-ccm.yml
#       dest: "{{ rke_config_dir }}/openstack-ccm.yml"
#       mode: u=rw,g=r,o=
#     register: ccm_conf

#   - name: Install cloud provider openstack helm repo
#     kubernetes.core.helm_repository:
#       name: cpo
#       repo_url: https://kubernetes.github.io/cloud-provider-openstack

#   - name: Install OCCM
#     kubernetes.core.helm: 
#       name: openstack-ccm 
#       chart_ref: cpo/openstack-cloud-controller-manager
#       release_namespace: kube-system 
#       values_files: 
#         - "{{ rke_config_dir }}/openstack-ccm.yml"
    
#   - name: Install cinder-csi-plugin
#     kubernetes.core.helm: 
#       name: cinder-csi
#       chart_ref: cpo/openstack-cinder-csi
#       release_namespace: kube-system 



